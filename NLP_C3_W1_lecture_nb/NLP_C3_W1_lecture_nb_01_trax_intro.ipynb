{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trax : Ungraded Lecture Notebook\n",
    "\n",
    "In this notebook you'll get to know about the Trax framework and learn about some of its basic building blocks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Why Trax and not TensorFlow or PyTorch?\n",
    "\n",
    "TensorFlow and PyTorch are both extensive frameworks that can do almost anything in deep learning. They offer a lot of flexibility, but that often means verbosity of syntax and extra time to code.\n",
    "\n",
    "Trax is much more concise. It runs on a TensorFlow backend but allows you to train models with 1 line commands. Trax also runs end to end, allowing you to get data, model and train all with a single terse statements. This means you can focus on learning, instead of spending hours on the idiosyncrasies of big framework implementation.\n",
    "\n",
    "### Why not Keras then?\n",
    "\n",
    "Keras is now part of Tensorflow itself from 2.0 onwards. Also, trax is good for implementing new state of the art algorithms like Transformers, Reformers, BERT because it is actively maintained by Google Brain Team for advanced deep learning tasks. It runs smoothly on CPUs,GPUs and TPUs as well with comparatively lesser modifications in code.\n",
    "\n",
    "### How to Code in Trax\n",
    "Building models in Trax relies on 2 key concepts:- **layers** and **combinators**.\n",
    "Trax layers are simple objects that process data and perform computations. They can be chained together into composite layers using Trax combinators, allowing you to build layers and models of any complexity.\n",
    "\n",
    "### Trax, JAX, TensorFlow and Tensor2Tensor\n",
    "\n",
    "You already know that Trax uses Tensorflow as a backend, but it also uses the JAX library to speed up computation too. You can view JAX as an enhanced and optimized version of numpy. \n",
    "\n",
    "**Watch out for assignments which import `import trax.fastmath.numpy as np`. If you see this line, remember that when calling `np` you are really calling Trax’s version of numpy that is compatible with JAX.**\n",
    "\n",
    "As a result of this, where you used to encounter the type `numpy.ndarray` now you will find the type `jax.interpreters.xla.DeviceArray`.\n",
    "\n",
    "Tensor2Tensor is another name you might have heard. It started as an end to end solution much like how Trax is designed, but it grew unwieldy and complicated. So you can view Trax as the new improved version that operates much faster and simpler.\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Trax source code can be found on Github: [Trax](https://github.com/google/trax)\n",
    "- JAX library: [JAX](https://jax.readthedocs.io/en/latest/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My insights:\n",
    "1. so layers intrax are like Dense, Activation, Embedding, Dot, Concatenate, Add, LSTM, RNN, layers in tensorflow\n",
    "2. combinators in trax are like the classes we use in tensorflow that allows us to chain the aforementioned layers, e.g. Model, Sequential etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Trax\n",
    "\n",
    "Trax has dependencies on JAX and some libraries like JAX which are yet to be supported in [Windows](https://github.com/google/jax/blob/1bc5896ee4eab5d7bb4ec6f161d8b2abb30557be/README.md#installation) but work well in Ubuntu and MacOS. We would suggest that if you are working on Windows, try to install Trax on WSL2. \n",
    "\n",
    "Official maintained documentation - [trax-ml](https://trax-ml.readthedocs.io/en/latest/) not to be confused with this [TraX](https://trax.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trax==1.3.9 Use this version for this notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\nlp-specialization-hw\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.ops' has no attribute 'index_add'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m  \u001b[39m# regular ol' numpy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m \u001b[39mimport\u001b[39;00m layers \u001b[39mas\u001b[39;00m tl  \u001b[39m# core building block\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m \u001b[39mimport\u001b[39;00m shapes  \u001b[39m# data signatures: dimensionality and type\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m \u001b[39mimport\u001b[39;00m fastmath  \u001b[39m# uses jax, offers numpy on steroids\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp-specialization-hw\\lib\\site-packages\\trax\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# coding=utf-8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Copyright 2021 The Trax Authors.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m\"\"\"Trax top level import.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m \u001b[39mimport\u001b[39;00m fastmath\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp-specialization-hw\\lib\\site-packages\\trax\\data\\__init__.py:36\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m\"\"\"Functions and classes for obtaining and preprocesing data.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[39mThe ``trax.data`` module presents a flattened (no subpackages) public API.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdebug_data_pipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m debug_pipeline\n\u001b[1;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minputs\u001b[39;00m \u001b[39mimport\u001b[39;00m add_loss_weights\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minputs\u001b[39;00m \u001b[39mimport\u001b[39;00m addition_inputs\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minputs\u001b[39;00m \u001b[39mimport\u001b[39;00m AddLossWeights\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp-specialization-hw\\lib\\site-packages\\trax\\data\\inputs.py:87\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m \u001b[39mimport\u001b[39;00m fastmath\n\u001b[0;32m     88\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m \u001b[39mimport\u001b[39;00m shapes\n\u001b[0;32m     89\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m debug_data_pipeline\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp-specialization-hw\\lib\\site-packages\\trax\\fastmath\\__init__.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfastmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m tree_leaves\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfastmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m tree_unflatten\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfastmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# pylint: disable=wildcard-import\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp-specialization-hw\\lib\\site-packages\\trax\\fastmath\\ops.py:36\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39menum\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgin\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfastmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m JAX_BACKEND\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfastmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m NUMPY_BACKEND\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfastmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf\u001b[39;00m \u001b[39mimport\u001b[39;00m TF_BACKEND\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp-specialization-hw\\lib\\site-packages\\trax\\fastmath\\jax.py:192\u001b[0m\n\u001b[0;32m    168\u001b[0m   _f\u001b[39m.\u001b[39mdefvjp(f_fwd, f_bwd)\n\u001b[0;32m    169\u001b[0m   \u001b[39mreturn\u001b[39;00m _f\n\u001b[0;32m    172\u001b[0m JAX_BACKEND \u001b[39m=\u001b[39m {\n\u001b[0;32m    173\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mjax\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    174\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m'\u001b[39m: jnp,\n\u001b[0;32m    175\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mabstract_eval\u001b[39m\u001b[39m'\u001b[39m: jax_abstract_eval,\n\u001b[0;32m    176\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mavg_pool\u001b[39m\u001b[39m'\u001b[39m: jax_avg_pool,\n\u001b[0;32m    177\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcond\u001b[39m\u001b[39m'\u001b[39m: lax\u001b[39m.\u001b[39mcond,\n\u001b[0;32m    178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mconv\u001b[39m\u001b[39m'\u001b[39m: jax_conv,\n\u001b[0;32m    179\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcustom_vjp\u001b[39m\u001b[39m'\u001b[39m: _custom_vjp,\n\u001b[0;32m    180\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcustom_grad\u001b[39m\u001b[39m'\u001b[39m: _custom_grad,\n\u001b[0;32m    181\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdataset_as_numpy\u001b[39m\u001b[39m'\u001b[39m: _dataset_as_numpy,\n\u001b[0;32m    182\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdynamic_slice\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mlax\u001b[39m.\u001b[39mdynamic_slice,\n\u001b[0;32m    183\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdynamic_slice_in_dim\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mlax\u001b[39m.\u001b[39mdynamic_slice_in_dim,\n\u001b[0;32m    184\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdynamic_update_slice\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mlax\u001b[39m.\u001b[39mdynamic_update_slice,\n\u001b[0;32m    185\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdynamic_update_slice_in_dim\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mlax\u001b[39m.\u001b[39mdynamic_update_slice_in_dim,\n\u001b[0;32m    186\u001b[0m     \u001b[39m'\u001b[39m\u001b[39merf\u001b[39m\u001b[39m'\u001b[39m: jax_special\u001b[39m.\u001b[39merf,\n\u001b[0;32m    187\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexpit\u001b[39m\u001b[39m'\u001b[39m: jax_special\u001b[39m.\u001b[39mexpit,\n\u001b[0;32m    188\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfori_loop\u001b[39m\u001b[39m'\u001b[39m: lax\u001b[39m.\u001b[39mfori_loop,\n\u001b[0;32m    189\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mglobal_device_count\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mdevice_count,\n\u001b[0;32m    190\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mgrad\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mgrad,\n\u001b[0;32m    191\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mvalue_and_grad\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mvalue_and_grad,\n\u001b[1;32m--> 192\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mindex_add\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mindex_add,\n\u001b[0;32m    193\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mindex_max\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mindex_max,\n\u001b[0;32m    194\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mindex_min\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mindex_min,\n\u001b[0;32m    195\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mindex_update\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mindex_update,\n\u001b[0;32m    196\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mjit\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mjit,\n\u001b[0;32m    197\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlocal_device_count\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mlocal_device_count,\n\u001b[0;32m    198\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlogsumexp\u001b[39m\u001b[39m'\u001b[39m: jax_special\u001b[39m.\u001b[39mlogsumexp,\n\u001b[0;32m    199\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlt\u001b[39m\u001b[39m'\u001b[39m: lax\u001b[39m.\u001b[39mlt,\n\u001b[0;32m    200\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m'\u001b[39m: lax\u001b[39m.\u001b[39mmap,\n\u001b[0;32m    201\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_pool\u001b[39m\u001b[39m'\u001b[39m: jax_max_pool,\n\u001b[0;32m    202\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpmap\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mpmap,\n\u001b[0;32m    203\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpsum\u001b[39m\u001b[39m'\u001b[39m: lax\u001b[39m.\u001b[39mpsum,\n\u001b[0;32m    204\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mrandom_bernoulli\u001b[39m\u001b[39m'\u001b[39m: jax_random\u001b[39m.\u001b[39mbernoulli,\n\u001b[0;32m    205\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mrandom_get_prng\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mjit(jax_random\u001b[39m.\u001b[39mPRNGKey),\n\u001b[0;32m    206\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mrandom_normal\u001b[39m\u001b[39m'\u001b[39m: jax_random\u001b[39m.\u001b[39mnormal,\n\u001b[0;32m    207\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mrandom_randint\u001b[39m\u001b[39m'\u001b[39m: jax_randint,\n\u001b[0;32m    208\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mrandom_split\u001b[39m\u001b[39m'\u001b[39m: jax_random\u001b[39m.\u001b[39msplit,\n\u001b[0;32m    209\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mrandom_fold_in\u001b[39m\u001b[39m'\u001b[39m: jax_random\u001b[39m.\u001b[39mfold_in,\n\u001b[0;32m    210\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mrandom_uniform\u001b[39m\u001b[39m'\u001b[39m: jax_random\u001b[39m.\u001b[39muniform,\n\u001b[0;32m    211\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mremat\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mremat,\n\u001b[0;32m    212\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mscan\u001b[39m\u001b[39m'\u001b[39m: lax\u001b[39m.\u001b[39mscan,\n\u001b[0;32m    213\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msort_key_val\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mlax\u001b[39m.\u001b[39msort_key_val,\n\u001b[0;32m    214\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mstop_gradient\u001b[39m\u001b[39m'\u001b[39m: lax\u001b[39m.\u001b[39mstop_gradient,\n\u001b[0;32m    215\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msum_pool\u001b[39m\u001b[39m'\u001b[39m: jax_sum_pool,\n\u001b[0;32m    216\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m'\u001b[39m: lax\u001b[39m.\u001b[39mtop_k,\n\u001b[0;32m    217\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mvjp,\n\u001b[0;32m    218\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mvmap\u001b[39m\u001b[39m'\u001b[39m: jax\u001b[39m.\u001b[39mvmap,\n\u001b[0;32m    219\u001b[0m }\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'jax.ops' has no attribute 'index_add'"
     ]
    }
   ],
   "source": [
    "import numpy as np  # regular ol' numpy\n",
    "\n",
    "from trax import layers as tl  # core building block\n",
    "from trax import shapes  # data signatures: dimensionality and type\n",
    "from trax import fastmath  # uses jax, offers numpy on steroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nlp-specialization-hw (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Trax version 1.3.9 or better \n",
    "!pip list | grep trax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "Layers are the core building blocks in Trax or as mentioned in the lectures, they are the base classes.\n",
    "\n",
    "They take inputs, compute functions/custom calculations and return outputs.\n",
    "\n",
    "You can also inspect layer properties. Let me show you some examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu Layer\n",
    "First let's see how to build a relu activation function as a layer. A layer like this is one of the simplest types. Notice there is no object initialization so it works just like a math function.\n",
    "\n",
    "**Note: Activation functions are also layers in Trax, which might look odd if you have been using other frameworks for a longer time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nlp-specialization-hw (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Layers\n",
    "# Create a relu trax layer\n",
    "relu = tl.Relu()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", relu.name)\n",
    "print(\"expected inputs :\", relu.n_in)\n",
    "print(\"promised outputs :\", relu.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = relu(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Layer\n",
    "Now let's check how to build a layer that takes 2 inputs. Notice the change in the expected inputs property from 1 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nlp-specialization-hw (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create a concatenate trax layer\n",
    "concat = tl.Concatenate()\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", concat.name)\n",
    "print(\"expected inputs :\", concat.n_in)\n",
    "print(\"promised outputs :\", concat.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x1 :\", x1)\n",
    "print(\"x2 :\", x2, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = concat([x1, x2])\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers are Configurable\n",
    "You can change the default settings of layers. For example, you can change the expected inputs for a concatenate layer from 2 to 3 using the optional parameter `n_items`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nlp-specialization-hw (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Configure a concatenate layer\n",
    "concat_3 = tl.Concatenate(n_items=3)  # configure the layer's expected inputs\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", concat_3.name)\n",
    "print(\"expected inputs :\", concat_3.n_in)\n",
    "print(\"promised outputs :\", concat_3.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "x3 = x2 * 0.99\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x1 :\", x1)\n",
    "print(\"x2 :\", x2)\n",
    "print(\"x3 :\", x3, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = concat_3([x1, x2, x3])\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: At any point,if you want to refer the function help/ look up the [documentation](https://trax-ml.readthedocs.io/en/latest/) or use help function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nlp-specialization-hw (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#help(tl.Concatenate) #Uncomment this to see the function docstring with explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers can have Weights\n",
    "Some layer types include mutable weights and biases that are used in computation and training. Layers of this type require initialization before use.\n",
    "\n",
    "For example the `LayerNorm` layer calculates normalized data, that is also scaled by weights and biases. During initialization you pass the data shape and data type of the inputs, so the layer can initialize compatible arrays of weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nlp-specialization-hw (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Uncomment any of them to see information regarding the function\n",
    "# help(tl.LayerNorm)\n",
    "# help(shapes.signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nlp-specialization-hw (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Layer initialization\n",
    "norm = tl.LayerNorm()\n",
    "# You first must know what the input data will look like\n",
    "x = np.array([0, 1, 2, 3], dtype=\"float\")\n",
    "\n",
    "# Use the input data signature to get shape and type for initializing weights and biases\n",
    "norm.init(shapes.signature(x)) # We need to convert the input datatype from usual tuple to trax ShapeDtype\n",
    "\n",
    "print(\"Normal shape:\",x.shape, \"Data Type:\",type(x.shape))\n",
    "print(\"Shapes Trax:\",shapes.signature(x),\"Data Type:\",type(shapes.signature(x)))\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", norm.name)\n",
    "print(\"expected inputs :\", norm.n_in)\n",
    "print(\"promised outputs :\", norm.n_out)\n",
    "# Weights and biases\n",
    "print(\"weights :\", norm.weights[0])\n",
    "print(\"biases :\", norm.weights[1], \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x)\n",
    "\n",
    "# Outputs\n",
    "y = norm(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layers\n",
    "This is where things start getting more interesting!\n",
    "You can create your own custom layers too and define custom functions for computations by using `tl.Fn`. Let me show you how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nlp-specialization-hw (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# help(tl.Fn) # Uncomment to see information regarding the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nlp-specialization-hw (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define a custom layer\n",
    "# In this example you will create a layer to calculate the input times 2\n",
    "\n",
    "def TimesTwo():\n",
    "    layer_name = \"TimesTwo\" #don't forget to give your custom layer a name to identify\n",
    "\n",
    "    # Custom function for the custom layer\n",
    "    def func(x):\n",
    "        return x * 2\n",
    "\n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "\n",
    "# Test it\n",
    "times_two = TimesTwo()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", times_two.name)\n",
    "print(\"expected inputs :\", times_two.n_in)\n",
    "print(\"promised outputs :\", times_two.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([1, 2, 3])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = times_two(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Insights:\n",
    "1. t1.Fn(\"<layer name or the defined class' name>\", <the custom function to use for the custom layer>) is almost similar to inheriting from the tf.keras.layers.Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinators\n",
    "You can combine layers to build more complex layers. Trax provides a set of objects named combinator layers to make this happen. Combinators are themselves layers, so behavior commutes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial Combinator\n",
    "This is the most common and easiest to use. For example could build a simple neural network by combining layers into a single layer using the `Serial` combinator. This new layer then acts just like a single layer, so you can inspect intputs, outputs and weights. Or even combine it into another layer! Combinators can then be used as trainable models. _Try adding more layers_\n",
    "\n",
    "**Note: As you must have guessed, if there is serial combinator, there must be a parallel combinator as well. Do try to explore about combinators and other layers from the trax documentation and look at the repo to understand how these layers are written.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nlp-specialization-hw (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Uncomment any of them to see information regarding the function\n",
    "# help(tl.Serial)\n",
    "# help(tl.Parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nlp-specialization-hw (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Serial combinator\n",
    "serial = tl.Serial(\n",
    "    tl.LayerNorm(),         # normalize input\n",
    "    tl.Relu(),              # convert negative values to zero\n",
    "    times_two,              # the custom layer you created above, multiplies the input recieved from above by 2\n",
    "    \n",
    "    ### START CODE HERE\n",
    "#     tl.Dense(n_units=2),  # try adding more layers. eg uncomment these lines\n",
    "#     tl.Dense(n_units=1),  # Binary classification, maybe? uncomment at your own peril\n",
    "#     tl.LogSoftmax()       # Yes, LogSoftmax is also a layer\n",
    "    ### END CODE HERE\n",
    ")\n",
    "\n",
    "# Initialization\n",
    "x = np.array([-2, -1, 0, 1, 2]) #input\n",
    "serial.init(shapes.signature(x)) #initialising serial instance\n",
    "\n",
    "print(\"-- Serial Model --\")\n",
    "print(serial,\"\\n\")\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", serial.name)\n",
    "print(\"sublayers :\", serial.sublayers)\n",
    "print(\"expected inputs :\", serial.n_in)\n",
    "print(\"promised outputs :\", serial.n_out)\n",
    "print(\"weights & biases:\", serial.weights, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = serial(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Insights:\n",
    "1. like layers in tensorflow, again layers in trax have a call() method for the class object or a layer's class itself. E.g. layer_1Z Dense(units=16, activation='relu') is called through layer_1Z(input)\n",
    "\n",
    "concat_layer = Concatenate(n_inputs=3) is called through concat_layer([input_1, input_2, input3]) (however layers like this where in tensorflow it was needed to specify the axis to in this case concatenate the two arrays/tensors in trax however I wuld assume you don't need to understand anymore theser kinds of concepts since it already does it in the background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX\n",
    "Just remember to lookout for which numpy you are using, the regular ol' numpy or Trax's JAX compatible numpy. Both tend to use the alias np so watch those import blocks.\n",
    "\n",
    "**Note: There are certain things which are still not possible in fastmath.numpy which can be done in numpy so you will see in assignments we will switch between them to get our work done.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nlp-specialization-hw (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Numpy vs fastmath.numpy have different data types\n",
    "# Regular ol' numpy\n",
    "x_numpy = np.array([1, 2, 3])\n",
    "print(\"good old numpy : \", type(x_numpy), \"\\n\")\n",
    "\n",
    "# Fastmath and jax numpy\n",
    "x_jax = fastmath.numpy.array([1, 2, 3])\n",
    "print(\"jax trax numpy : \", type(x_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Trax is a concise framework, built on TensorFlow, for end to end machine learning. The key building blocks are layers and combinators. This notebook is just a taste, but sets you up with some key inuitions to take forward into the rest of the course and assignments where you will build end to end models."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
